// Lexer for Wyn written in Wyn
// Proof of concept for self-hosting

enum TokenType {
    EndOfFile,
    Ident,
    Int,
    String,
    
    // Keywords
    Fn,
    Var,
    If,
    Else,
    While,
    Return,
    
    // Operators
    Plus,
    Minus,
    Star,
    Slash,
    Equal,
    EqualEqual,
    Less,
    Greater,
    Arrow,
    Dot,
    Colon,
    
    // Delimiters
    LParen,
    RParen,
    LBrace,
    RBrace,
    Comma,
    Semicolon,
    
    Unknown
}

struct Token {
    tok_type: TokenType,
    value: string
}

fn is_digit(ch: string) -> int {
    if ch == "0" { return 1 }
    if ch == "1" { return 1 }
    if ch == "2" { return 1 }
    if ch == "3" { return 1 }
    if ch == "4" { return 1 }
    if ch == "5" { return 1 }
    if ch == "6" { return 1 }
    if ch == "7" { return 1 }
    if ch == "8" { return 1 }
    if ch == "9" { return 1 }
    return 0
}

fn is_alpha(ch: string) -> int {
    // Simplified: just check a few letters
    if ch == "a" { return 1 }
    if ch == "b" { return 1 }
    if ch == "f" { return 1 }
    if ch == "n" { return 1 }
    if ch == "r" { return 1 }
    if ch == "v" { return 1 }
    if ch == "x" { return 1 }
    if ch == "y" { return 1 }
    // TODO: Add full alphabet
    return 0
}

fn is_whitespace(ch: string) -> int {
    if ch == " " { return 1 }
    if ch == "\n" { return 1 }
    if ch == "\t" { return 1 }
    return 0
}

fn check_keyword(input: string, start: int, len: int) -> TokenType {
    // Returns TokenType for keyword, or TokenType_Ident if not a keyword
    
    // Check "fn" (length 2)
    if len == 2 {
        if input[start] == "f" {
            if input[start + 1] == "n" {
                return TokenType_Fn
            }
        }
    }
    
    // Check "if" (length 2)
    if len == 2 {
        if input[start] == "i" {
            if input[start + 1] == "f" {
                return TokenType_If
            }
        }
    }
    
    // Check "var" (length 3)
    if len == 3 {
        if input[start] == "v" {
            if input[start + 1] == "a" {
                if input[start + 2] == "r" {
                    return TokenType_Var
                }
            }
        }
    }
    
    // Not a keyword
    return TokenType_Ident
}

fn substr(s: string, start: int, len: int) -> string {
    var result = ""
    var i = 0
    while i < len {
        result = result + s[start + i]
        i = i + 1
    }
    return result
}

fn make_token(tok_type: TokenType, value: string) -> Token {
    var tok: Token = Token { tok_type: tok_type, value: value }
    return tok
}

fn lex(input: string) -> [Token] {
    var pos = 0
    var tokens: [Token] = []
    
    while pos < input.len() {
        var ch = input[pos]
        
        if is_whitespace(ch) {
            pos = pos + 1
        } else if is_digit(ch) {
            var start = pos
            while pos < input.len() {
                if is_digit(input[pos]) {
                    pos = pos + 1
                } else {
                    break
                }
            }
            tokens.push(make_token(TokenType_Int, substr(input, start, pos - start)))
        } else if is_alpha(ch) {
            var start = pos
            while pos < input.len() {
                var c = input[pos]
                if is_alpha(c) {
                    pos = pos + 1
                } else {
                    break
                }
            }
            tokens.push(make_token(check_keyword(input, start, pos - start), substr(input, start, pos - start)))
        } else if ch == "+" {
            tokens.push(make_token(TokenType_Plus, "+"))
            pos = pos + 1
        } else if ch == "-" {
            tokens.push(make_token(TokenType_Minus, "-"))
            pos = pos + 1
        } else if ch == "*" {
            tokens.push(make_token(TokenType_Star, "*"))
            pos = pos + 1
        } else if ch == "/" {
            tokens.push(make_token(TokenType_Slash, "/"))
            pos = pos + 1
        } else if ch == "=" {
            tokens.push(make_token(TokenType_Equal, "="))
            pos = pos + 1
        } else if ch == "<" {
            tokens.push(make_token(TokenType_Less, "<"))
            pos = pos + 1
        } else if ch == ">" {
            tokens.push(make_token(TokenType_Greater, ">"))
            pos = pos + 1
        } else if ch == "." {
            tokens.push(make_token(TokenType_Dot, "."))
            pos = pos + 1
        } else if ch == ":" {
            tokens.push(make_token(TokenType_Colon, ":"))
            pos = pos + 1
        } else if ch == "(" {
            tokens.push(make_token(TokenType_LParen, "("))
            pos = pos + 1
        } else if ch == ")" {
            tokens.push(make_token(TokenType_RParen, ")"))
            pos = pos + 1
        } else if ch == "{" {
            tokens.push(make_token(TokenType_LBrace, "{"))
            pos = pos + 1
        } else if ch == "}" {
            tokens.push(make_token(TokenType_RBrace, "}"))
            pos = pos + 1
        } else if ch == ";" {
            tokens.push(make_token(TokenType_Semicolon, ";"))
            pos = pos + 1
        } else {
            tokens.push(make_token(TokenType_Unknown, ch))
            pos = pos + 1
        }
    }
    
    tokens.push(make_token(TokenType_EndOfFile, ""))
    return tokens
}

fn main() -> int {
    var input = "var x = 42"
    var tokens = lex(input)
    
    print("Tokenization test:")
    print("Input: var x = 42")
    print("Token count: ")
    print(tokens.len())
    
    if tokens.len() == 5 {
        print("✓ Lexer tokenization works!")
        return 0
    }
    
    print("✗ Expected 5 tokens")
    return 1
}
